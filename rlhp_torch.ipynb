{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning from Human Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self):\n",
    "        self.state = self.init_state()\n",
    "        self.step_count = 0\n",
    "\n",
    "    def init_state(self):\n",
    "        xs = random.sample(range(0, 7), 5)\n",
    "        ys = random.sample(range(0, 7), 5)\n",
    "        piece1 = (xs[0], ys[0])\n",
    "        playerpos = (xs[4], ys[4])\n",
    "        positions = list(np.array([playerpos, piece1]).flatten())\n",
    "        return positions\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.init_state()\n",
    "        self.step_count = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        if action == 0:    # UP\n",
    "            self.state[1] += 1\n",
    "        elif action == 1:  # RIGHT\n",
    "            self.state[0] += 1\n",
    "        elif action == 2:  # DOWN\n",
    "            self.state[1] -= 1\n",
    "        elif action == 3:  # LEFT\n",
    "            self.state[0] -= 1\n",
    "        \n",
    "        if self.step_count >= 25:\n",
    "            self.reset()\n",
    "            reward = int(rewarder(torch.tensor(self.state, dtype=torch.float32).unsqueeze(0)).item())\n",
    "            playing = False\n",
    "            return reward, playing\n",
    "        \n",
    "        if self.state[0] == self.state[2] and self.state[1] == self.state[3]:\n",
    "            playing = False\n",
    "            reward = int(rewarder(torch.tensor(self.state, dtype=torch.float32).unsqueeze(0)).item())\n",
    "            self.reset()\n",
    "            return playing, reward\n",
    "        \n",
    "        playing = True\n",
    "        reward = int(rewarder(torch.tensor(self.state, dtype=torch.float32).unsqueeze(0)).item())\n",
    "        return reward, playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_size,num_actions, num_hidden_units):\n",
    "        super(Actor, self).__init__()\n",
    "        self.shared_1 = nn.Linear(in_size, num_hidden_units)\n",
    "        self.actor = nn.Linear(num_hidden_units, num_actions)\n",
    "\n",
    "    def forward(self, input_obs):\n",
    "        x = F.relu(self.shared_1(input_obs))\n",
    "        return self.actor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewarder(nn.Module):\n",
    "    def __init__(self, in_size, num_hidden_units):\n",
    "        super(Rewarder, self).__init__()\n",
    "        self.shared_1 = nn.Linear(in_size, num_hidden_units)\n",
    "        self.reward = nn.Linear(num_hidden_units, 1)\n",
    "\n",
    "    def forward(self, input_obs):\n",
    "        x = F.relu(self.shared_1(input_obs))\n",
    "        return self.reward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Actor(4, 4, 100)\n",
    "rewarder = Rewarder(4, 100)\n",
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_g(reward_trajectory, gamma):\n",
    "    ez_discount = np.array([gamma ** n for n in range(len(reward_trajectory))])\n",
    "    gs = []\n",
    "    reward_trajectory = np.array(reward_trajectory)\n",
    "    for ts in range(len(reward_trajectory)):\n",
    "        to_end_rewards = reward_trajectory[ts:]\n",
    "        eq_len_discount = ez_discount[:len(reward_trajectory[ts:])]\n",
    "        total_value = np.multiply(to_end_rewards, eq_len_discount)\n",
    "        g = sum(total_value)\n",
    "        gs.append(g)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_episode(env, model):\n",
    "    env.reset()\n",
    "    action_probs_list = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    playing = True\n",
    "    while playing:\n",
    "        obs = torch.tensor(env.state, dtype=torch.float32).unsqueeze(0)\n",
    "        action_logits = agent(obs)\n",
    "        action_probs = F.softmax(action_logits, dim=-1)\n",
    "        selected_action_idx = torch.multinomial(action_probs, 1).item()\n",
    "        states.append(obs)\n",
    "        actions.append(selected_action_idx)\n",
    "\n",
    "        reward, playing = env.step(selected_action_idx)\n",
    "\n",
    "        probability_of_taking_selected_action = action_probs[0, selected_action_idx]\n",
    "        action_probs_list.append(probability_of_taking_selected_action)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return action_probs_list, rewards, states, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_loss(action_probs, rewards):\n",
    "    gs = calculate_g(rewards, 0.99)\n",
    "    action_log_probs = torch.log(torch.stack(action_probs))\n",
    "    loss = -torch.sum(action_log_probs * torch.tensor(gs, dtype=torch.float32))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_rewarder = optim.Adam(rewarder.parameters(), lr=0.0005)\n",
    "optimizer_actor = optim.Adam(agent.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_action(action):\n",
    "    actions = {\n",
    "        0: 'up',\n",
    "        1: 'right',\n",
    "        2: 'down',\n",
    "        3: 'left'\n",
    "    }\n",
    "    return actions[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(transition_ids, states, actions):\n",
    "    d1xs = [states[transition_ids[0]][0][0].item(), states[transition_ids[0]][0][2].item()]\n",
    "    d1ys = [states[transition_ids[0]][0][1].item(), states[transition_ids[0]][0][3].item()]\n",
    "    d2xs = [states[transition_ids[1]][0][0].item(), states[transition_ids[1]][0][2].item()]\n",
    "    d2ys = [states[transition_ids[1]][0][1].item(), states[transition_ids[1]][0][3].item()]\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2)\n",
    "    color = ['yellow', 'green']\n",
    "    color_indices = [0, 1]\n",
    "    colormap = matplotlib.colors.ListedColormap(color)\n",
    "    ax[0, 0].scatter(d1xs, d1ys, c=color_indices, cmap=colormap)\n",
    "    ax[0, 0].set_title(str(decode_action(actions[transition_ids[0]])))\n",
    "    ax[0, 1].scatter(d2xs, d2ys, c=color_indices, cmap=colormap)\n",
    "    ax[0, 1].set_title(str(decode_action(actions[transition_ids[1]])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preference_update(states, actions, rewarder):\n",
    "    transition_ids = random.sample(range(len(states)-2), 2)\n",
    "    compare(transition_ids, states, actions)\n",
    "    pref = input('select preference a: left, d: right, s: same  ')\n",
    "    dists = {'a': [1, 0], 'd': [0, 1], 's': [0.5, 0.5]}\n",
    "    dist = dists[pref]\n",
    "\n",
    "    reward1 = rewarder(states[transition_ids[0]+1])\n",
    "    reward2 = rewarder(states[transition_ids[1]+1])\n",
    "    p1 = torch.exp(reward1) / (torch.exp(reward1) + torch.exp(reward2))\n",
    "    p2 = torch.exp(reward2) / (torch.exp(reward1) + torch.exp(reward2))\n",
    "    loss = -torch.log(p1) * dist[0] - torch.log(p2) * dist[1]\n",
    "    \n",
    "    optimizer_rewarder.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_rewarder.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_length = []\n",
    "for episode in range(5000):\n",
    "    action_probs, rewards, states, actions = step_episode(env, agent)\n",
    "    loss = actor_loss(action_probs, rewards)\n",
    "    \n",
    "    optimizer_actor.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_actor.step()\n",
    "    \n",
    "    average_length.append(len(rewards))\n",
    "    \n",
    "    preference_update(states, actions, rewarder)\n",
    "    \n",
    "    print('Episode:', episode, '\\nAverage steps to target:', np.mean(average_length[-100:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cistup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
